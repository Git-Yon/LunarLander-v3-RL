{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623e2644",
   "metadata": {},
   "source": [
    "# LunarLander with RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d94ea9",
   "metadata": {},
   "source": [
    "## 1️⃣ Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Présentation du projet et contexte\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b1fa9",
   "metadata": {},
   "source": [
    "Ce projet a pour objectif d'explorer le **reinforcement learning (apprentissage par renforcement)** à travers un environnement classique : **LunarLander-v2**.\n",
    "\n",
    "LunarLander est un simulateur dans lequel un module spatial doit atterrir en douceur sur la surface lunaire, en contrôlant ses moteurs latéraux et principal. Cet environnement propose un bon compromis entre complexité et accessibilité, idéal pour tester différents algorithmes d'apprentissage par renforcement.\n",
    "\n",
    "Dans ce notebook, nous allons entraîner un agent intelligent capable de maîtriser ce jeu à l'aide d'algorithmes modernes comme **PPO (Proximal Policy Optimization)** et **DQN (Deep Q-Network)**. \n",
    "\n",
    "Le but est à la fois théorique — comprendre les concepts clés du RL — et pratique — obtenir un agent performant avec visualisation des résultats.\n",
    "\n",
    "Ce travail sera ensuite exploitable comme un projet de portfolio sur GitHub, démontrant la maîtrise des techniques RL et la capacité à mettre en œuvre des solutions concrètes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa1c12",
   "metadata": {},
   "source": [
    "### Explication théorique du Reinforcement Learning (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a389313",
   "metadata": {},
   "source": [
    "Le **Reinforcement Learning (RL)** est une méthode d’apprentissage automatique où un agent apprend à prendre des décisions optimales en interagissant avec un environnement.\n",
    "\n",
    "Le processus se déroule en boucle :  \n",
    "- L’agent observe un **état** (`state`) de l’environnement.  \n",
    "- Il choisit une **action** (`action`) à effectuer.  \n",
    "- L’environnement répond en fournissant une **récompense** (`reward`) et un nouvel état.  \n",
    "\n",
    "L’objectif de l’agent est de maximiser la somme des récompenses qu’il reçoit sur le long terme, en apprenant une **politique** (`policy`) qui associe états et actions de manière optimale.\n",
    "\n",
    "Ce cadre est formalisé par un **Processus de Décision Markovien (MDP)**, où :  \n",
    "- `S` est l’ensemble des états possibles,  \n",
    "- `A` est l’ensemble des actions possibles,  \n",
    "- `P(s'|s,a)` est la probabilité de transition entre états,  \n",
    "- `R(s,a)` est la récompense obtenue après action `a` en état `s`.\n",
    "\n",
    "Les algorithmes de reinforcement learning se répartissent généralement en trois grandes catégories, selon ce qu’ils apprennent :\n",
    "\n",
    "#### 1. Les méthodes **Value-based**  \n",
    "Ces algorithmes apprennent une **fonction valeur**, c’est-à-dire une estimation de la \"qualité\" d’un état ou d’une action.  \n",
    "- **Q-learning** et **Deep Q-Network (DQN)** sont des exemples emblématiques.  \n",
    "- Ils calculent une fonction \\( Q(s, a) \\) qui estime la valeur attendue (cumulée des récompenses futures) de prendre une action \\( a \\) dans un état \\( s \\).  \n",
    "- La politique optimale est ensuite dérivée en choisissant l’action qui maximise \\( Q(s, a) \\).  \n",
    "- Ces méthodes fonctionnent principalement dans des espaces d’action **discrets**.\n",
    "\n",
    "#### 2. Les méthodes **Policy-based**  \n",
    "Ces algorithmes apprennent directement une **politique** \\( \\pi(a|s) \\) qui donne la probabilité de choisir une action \\( a \\) dans un état \\( s \\).  \n",
    "- Par exemple, **REINFORCE** et **Proximal Policy Optimization (PPO)**.  \n",
    "- Elles optimisent la politique par gradient, en cherchant à maximiser la récompense cumulée.  \n",
    "- Ces méthodes sont souvent plus stables et adaptées aux environnements avec des espaces d’action **discrets ou continus**.\n",
    "\n",
    "#### 3. Les méthodes **Actor-Critic**  \n",
    "Ce sont des méthodes hybrides qui combinent les deux approches précédentes :  \n",
    "- L’**actor** apprend la politique (comme en policy-based)  \n",
    "- Le **critic** apprend une fonction valeur (comme en value-based) qui guide la mise à jour de la politique  \n",
    "- Exemples : **Advantage Actor-Critic (A2C)**, **Deep Deterministic Policy Gradient (DDPG)**  \n",
    "- Elles sont particulièrement efficaces dans des environnements à actions **continues**.\n",
    "\n",
    "---\n",
    "\n",
    "Dans ce projet, nous allons commencer par entraîner un agent avec **PPO**, un algorithme policy-based réputé pour sa stabilité et ses bonnes performances. Ensuite, nous comparerons avec **DQN**, un algorithme value-based classique, pour voir leurs différences en pratique sur LunarLander.\n",
    "\n",
    "Ce mélange te donnera une bonne compréhension des deux grandes familles d’algorithmes RL !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639128b1",
   "metadata": {},
   "source": [
    "###  Présentation rapide de l’environnement LunarLander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180d1aa",
   "metadata": {},
   "source": [
    "LunarLander est un environnement de simulation développé par OpenAI Gym, inspiré du célèbre jeu où il faut faire atterrir en douceur un module spatial sur la surface de la Lune.\n",
    "\n",
    "**Description**\n",
    "\n",
    "- **Objectif** : Contrôler un module lunaire pour atterrir sans dommage sur une zone cible.  \n",
    "- **États** :  \n",
    "  L’environnement fournit un vecteur d’état de 8 dimensions comprenant :  \n",
    "  - La position horizontale et verticale du module  \n",
    "  - Sa vitesse horizontale et verticale  \n",
    "  - L’angle d’inclinaison et la vitesse angulaire  \n",
    "  - Deux indicateurs binaires indiquant si les pieds du module touchent le sol\n",
    "\n",
    "- **Actions** :  \n",
    "  L’environnement LunarLander-v2 propose 4 actions discrètes :  \n",
    "  1. Ne rien faire  \n",
    "  2. Allumer le moteur principal (poussée verticale)  \n",
    "  3. Allumer le moteur gauche (poussée latérale droite)  \n",
    "  4. Allumer le moteur droit (poussée latérale gauche)\n",
    "\n",
    "- **Récompenses** :  \n",
    "  - Récompense positive pour un atterrissage réussi et stable  \n",
    "  - Pénalités pour collisions violentes ou perte de contrôle  \n",
    "  - Bonus ou malus selon la consommation de carburant\n",
    "\n",
    "**Versions**\n",
    "\n",
    "- **LunarLander-v2** : version à actions discrètes (classique)  \n",
    "- **LunarLanderContinuous-v2** : version avec actions continues pour un contrôle plus fin des moteurs\n",
    "\n",
    "---\n",
    "\n",
    "Cet environnement est idéal pour expérimenter les algorithmes de reinforcement learning car il combine un contrôle précis et un challenge non trivial, tout en restant accessible aux débutants et intermédiaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9fa915",
   "metadata": {},
   "source": [
    "### Objectifs du notebook : apprentissage par PPO, DQN, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b782dfc2",
   "metadata": {},
   "source": [
    "Ce notebook a pour but de :\n",
    "\n",
    "- **Comprendre les principes fondamentaux du reinforcement learning** à travers une application concrète.  \n",
    "- **Mettre en œuvre et entraîner plusieurs algorithmes RL populaires**, notamment :  \n",
    "  - **PPO (Proximal Policy Optimization)**, un algorithme policy-based stable et performant.  \n",
    "  - **DQN (Deep Q-Network)**, un algorithme value-based classique efficace sur des espaces d’action discrets.  \n",
    "\n",
    "- **Comparer les performances et comportements des agents entraînés** avec ces méthodes sur l’environnement LunarLander.  \n",
    "- **Visualiser les résultats** à travers des simulations montrant les trajectoires d’atterrissage.  \n",
    "\n",
    "À la fin de ce notebook, tu auras une bonne compréhension des différences entre ces algorithmes, ainsi qu’une base solide pour expérimenter d’autres méthodes de RL.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cf5427",
   "metadata": {},
   "source": [
    "## 2️⃣ Installation et imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95def5a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Avant de commencer, il faut s’assurer que les packages nécessaires sont installés.\n",
    "\n",
    "```bash\n",
    "#Creation environnement et activation \n",
    "conda create -n rl_lander python=3.10\n",
    "conda activate rl_lander \n",
    "# Installation des packages via pip\n",
    "pip install swig\n",
    "pip install \"gymnasium[box2d]\"\n",
    "pip install stable-baselines3[extra]\n",
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6812c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pour visualiser les vidéos dans le notebook\n",
    "from IPython import display\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46960982",
   "metadata": {},
   "source": [
    "## 3️⃣ Exploration et test de l’environnement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116f19d",
   "metadata": {},
   "source": [
    "### 🔍 Description de l’environnement LunarLander-v3\n",
    "\n",
    "LunarLander est un environnement de type Box2D fourni par Gymnasium.\n",
    "\n",
    "L’objectif est d’atterrir en douceur sur une plateforme située au centre.  \n",
    "L'agent contrôle un atterrisseur lunaire avec :\n",
    "- Deux jambes pour stabiliser l’engin\n",
    "- Un moteur principal et deux moteurs latéraux\n",
    "\n",
    "#### ℹ️ Informations clés :\n",
    "\n",
    "- **Espace d’états (`observation_space`)** :  \n",
    "\n",
    "  Un vecteur `Box(8,)` contenant 8 valeurs :\n",
    "\n",
    "  - position x et y\n",
    "  - vitesse x et y\n",
    "  - angle\n",
    "  - vitesse angulaire\n",
    "  - contact jambe gauche (booléen)\n",
    "  - contact jambe droite (booléen)\n",
    "\n",
    "- **Espace d’actions (`action_space`)** :  \n",
    "\n",
    "  Discret avec 4 actions possibles :\n",
    "\n",
    "  0. Ne rien faire  \n",
    "  1. Feu principal (pousse vers le haut)  \n",
    "  2. Feu latéral droit (fait tourner à gauche)  \n",
    "  3. Feu latéral gauche (fait tourner à droite)\n",
    "\n",
    "---\n",
    "\n",
    "### 🎮 Test simple avec des actions aléatoires\n",
    "\n",
    "On lance une simulation avec des actions aléatoires pour observer le fonctionnement de l’environnement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07bd9c7",
   "metadata": {},
   "source": [
    "### Création de l’environnement LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8188e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward total de l’épisode : -178.96\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "# Création de l’environnement en mode image\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "obs, info = env.reset(seed=42)\n",
    "done = False\n",
    "total_reward = 0 \n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Action aléatoire\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "\n",
    "    # Affichage de l'image dans le notebook\n",
    "    frame = env.render()\n",
    "\n",
    "    plt.imshow(frame)\n",
    "    plt.axis(\"off\")\n",
    "    #display(plt.gcf())\n",
    "    clear_output(wait=True)\n",
    "    # pour ralentir l'animation\n",
    "\n",
    "env.close()\n",
    "print(f\"Reward total de l’épisode : {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d5ca7",
   "metadata": {},
   "source": [
    "## 4️⃣ Présentation des algorithmes choisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529f895",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Dans ce projet, nous allons expérimenter avec deux grandes familles d’algorithmes de Reinforcement Learning (RL) :\n",
    "\n",
    "- **DQN** (Deep Q-Network) — méthode Value-based\n",
    "- **PPO** (Proximal Policy Optimization) — méthode Policy-based (Actor-Critic)\n",
    "\n",
    "Ces deux approches sont populaires, performantes, et bien adaptées à des environnements comme LunarLander-v3.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 DQN (Deep Q-Network)\n",
    "\n",
    "DQN est une amélioration du Q-Learning classique, adaptée aux environnements à espace d’états **continu** grâce à l’utilisation d’un **réseau de neurones** pour approximer la fonction Q.\n",
    "\n",
    "Caractéristiques :\n",
    "- Basé sur l’estimation d’une **fonction valeur Q(s, a)**\n",
    "- Convient aux **actions discrètes** (ce qui est le cas de LunarLander)\n",
    "- Utilise une **replay buffer** pour stabiliser l’apprentissage\n",
    "- Nécessite un **réseau de neurones** comme estimateur\n",
    "\n",
    "✅ DQN fonctionne bien avec LunarLander car :\n",
    "- L’espace d’actions est **discret** (4 actions)\n",
    "- L’environnement est complexe mais maîtrisable\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 PPO (Proximal Policy Optimization)\n",
    "\n",
    "PPO fait partie des méthodes **Policy Gradient**, où l’agent apprend une politique directement.  \n",
    "C’est un algorithme **on-policy** robuste qui combine les avantages des méthodes Policy-based et Actor-Critic.\n",
    "\n",
    "Caractéristiques :\n",
    "- L’agent apprend une **politique π(a|s)** et une **fonction valeur V(s)**\n",
    "- Contrôle les mises à jour pour éviter des changements brutaux dans la politique\n",
    "- Très **stable** et **fiable**, même dans des environnements complexes\n",
    "- Fonctionne avec actions **discrètes ou continues**\n",
    "\n",
    "✅ PPO est un excellent choix pour LunarLander car :\n",
    "- Il offre une **meilleure convergence** que DQN dans certains cas\n",
    "- Il est plus **générique** (utilisable sur d'autres environnements plus complexes plus tard)\n",
    "\n",
    "---\n",
    "\n",
    "### ⚖️ Pourquoi ces choix pour LunarLander ?\n",
    "\n",
    "| Critère                     | DQN                        | PPO                             |\n",
    "|----------------------------|----------------------------|----------------------------------|\n",
    "| Type de méthode            | Value-based                | Policy-based (Actor-Critic)     |\n",
    "| Actions discrètes          | ✅ Oui                     | ✅ Oui                           |\n",
    "| Facilité d'implémentation  | ⭐⭐⭐                       | ⭐⭐⭐⭐                            |\n",
    "| Stabilité de l'entraînement| ⭐⭐                        | ⭐⭐⭐⭐                            |\n",
    "| Utilisation courante       | ✅                        | ✅✅ (très populaire)             |\n",
    "\n",
    "Nous allons donc tester **les deux** sur le même environnement pour :\n",
    "- Comparer les performances\n",
    "- Observer leur comportement\n",
    "- Comprendre leurs forces et limites\n",
    "\n",
    "---\n",
    "\n",
    "👉 D'autres algorithmes comme **A2C**, **DDPG**, ou **SAC** pourront être explorés plus tard, notamment pour des environnements à actions continues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a992411",
   "metadata": {},
   "source": [
    "## 5️⃣ 🔸 Entraînement avec PPO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb2c6d",
   "metadata": {},
   "source": [
    "### Création de l’agent PPO (avec explication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df888b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# 📦 Import du modèle PPO de stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# 📂 Création d'un dossier pour sauvegarder le modèle\n",
    "import os\n",
    "model_dir = \"models/PPO_LunarLander\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# ✅ Création de l'agent PPO\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",          # Réseau de neurones multilayer perceptron\n",
    "    env=env,                     # L'environnement LunarLander-v3\n",
    "    verbose=1,                   # Niveau de log (1 = infos, 0 = silence)\n",
    "    learning_rate=3e-4,          # Taux d’apprentissage (valeur standard efficace)\n",
    "    n_steps=2048,                # Taille du buffer de rollout\n",
    "    batch_size=64,               # Taille des mini-batchs pour l’update\n",
    "    n_epochs=10,                 # Nombre de passes sur les données par update\n",
    "    gamma=0.99,                  # Facteur de discount (récompenses futures)\n",
    "    gae_lambda=0.95,             # Avantage généralisé pour la variance/biais\n",
    "    tensorboard_log=\"./ppo_tensorboard/\"  # Pour suivi visuel (optionnel)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ddf91",
   "metadata": {},
   "source": [
    "### Lancement de l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c359b60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_tensorboard/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.3     |\n",
      "|    ep_rew_mean     | -174     |\n",
      "| time/              |          |\n",
      "|    fps             | 926      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 94           |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 730          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062142224 |\n",
      "|    clip_fraction        | 0.00903      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.00141      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 836          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00669     |\n",
      "|    value_loss           | 1.63e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 92.6        |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 676         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008383371 |\n",
      "|    clip_fraction        | 0.0353      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.00491     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 393         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00905    |\n",
      "|    value_loss           | 1.41e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 97          |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009753303 |\n",
      "|    clip_fraction        | 0.0589      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.0057     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 312         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 780         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 99.9        |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 631         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013677408 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | -0.035      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 140         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 753         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 102          |\n",
      "|    ep_rew_mean          | -180         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 620          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070023127 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | -0.00801     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 726          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00709     |\n",
      "|    value_loss           | 1.38e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 107         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 614         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015036378 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | -0.000353   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 342         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00631    |\n",
      "|    value_loss           | 756         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 114        |\n",
      "|    ep_rew_mean          | -165       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 589        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 27         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00932038 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.28      |\n",
      "|    explained_variance   | -0.000492  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 117        |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.00718   |\n",
      "|    value_loss           | 282        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 117         |\n",
      "|    ep_rew_mean          | -151        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 584         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006569542 |\n",
      "|    clip_fraction        | 0.0675      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | -5.35e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 213         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00565    |\n",
      "|    value_loss           | 494         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -133         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 585          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075313714 |\n",
      "|    clip_fraction        | 0.0707       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.000402     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 135          |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00826     |\n",
      "|    value_loss           | 298          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 126         |\n",
      "|    ep_rew_mean          | -113        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 584         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013742322 |\n",
      "|    clip_fraction        | 0.0766      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | -0.00522    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 80.7        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00833    |\n",
      "|    value_loss           | 239         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 139         |\n",
      "|    ep_rew_mean          | -109        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 572         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006172346 |\n",
      "|    clip_fraction        | 0.0222      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | -0.00699    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 214         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00614    |\n",
      "|    value_loss           | 386         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 145         |\n",
      "|    ep_rew_mean          | -104        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 571         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006491789 |\n",
      "|    clip_fraction        | 0.0492      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.0116      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 98.8        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 342         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 161         |\n",
      "|    ep_rew_mean          | -98.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 565         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002158649 |\n",
      "|    clip_fraction        | 0.00176     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.0879      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 405         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00157    |\n",
      "|    value_loss           | 537         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 170         |\n",
      "|    ep_rew_mean          | -93.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 566         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007315482 |\n",
      "|    clip_fraction        | 0.0492      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 52.9        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0048     |\n",
      "|    value_loss           | 145         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 179          |\n",
      "|    ep_rew_mean          | -83.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 565          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048648855 |\n",
      "|    clip_fraction        | 0.0136       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.0952       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 146          |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    value_loss           | 505          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 189         |\n",
      "|    ep_rew_mean          | -82         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 562         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015582055 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 58.8        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 170         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 205         |\n",
      "|    ep_rew_mean          | -80.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 555         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010640768 |\n",
      "|    clip_fraction        | 0.0446      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 165         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00595    |\n",
      "|    value_loss           | 264         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 217         |\n",
      "|    ep_rew_mean          | -81.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 554         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011312669 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.277       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 64.6        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 178         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 235         |\n",
      "|    ep_rew_mean          | -81.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008650876 |\n",
      "|    clip_fraction        | 0.0563      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.067       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 65.2        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00846    |\n",
      "|    value_loss           | 271         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 245          |\n",
      "|    ep_rew_mean          | -89.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 79           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068907565 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.211        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 112          |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00539     |\n",
      "|    value_loss           | 196          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 264         |\n",
      "|    ep_rew_mean          | -87.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 530         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004189785 |\n",
      "|    clip_fraction        | 0.0104      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.19        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 293         |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00386    |\n",
      "|    value_loss           | 495         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 276        |\n",
      "|    ep_rew_mean          | -83.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 520        |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 90         |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01212539 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.23      |\n",
      "|    explained_variance   | 0.495      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 26.9       |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    value_loss           | 81.9       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 302          |\n",
      "|    ep_rew_mean          | -82.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 510          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 96           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061505996 |\n",
      "|    clip_fraction        | 0.0651       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0.745        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.8         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00762     |\n",
      "|    value_loss           | 70.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 320         |\n",
      "|    ep_rew_mean          | -81.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 506         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015655426 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.2        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 52.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 331          |\n",
      "|    ep_rew_mean          | -79.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 105          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086736865 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.9         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    value_loss           | 37.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 348         |\n",
      "|    ep_rew_mean          | -77.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 499         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007969284 |\n",
      "|    clip_fraction        | 0.0507      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.582       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 93.8        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    value_loss           | 125         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 366         |\n",
      "|    ep_rew_mean          | -74.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 497         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004817497 |\n",
      "|    clip_fraction        | 0.0186      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 63.9        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00428    |\n",
      "|    value_loss           | 175         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 376         |\n",
      "|    ep_rew_mean          | -71         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 494         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009749742 |\n",
      "|    clip_fraction        | 0.0597      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00739    |\n",
      "|    value_loss           | 52.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 396         |\n",
      "|    ep_rew_mean          | -68.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 488         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 125         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015892874 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.588       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.1        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00789    |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 411         |\n",
      "|    ep_rew_mean          | -66.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 486         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007774596 |\n",
      "|    clip_fraction        | 0.0741      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.8        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00367    |\n",
      "|    value_loss           | 58.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 424          |\n",
      "|    ep_rew_mean          | -62.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 484          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 135          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050604744 |\n",
      "|    clip_fraction        | 0.0266       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.929       |\n",
      "|    explained_variance   | 0.593        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.5         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00373     |\n",
      "|    value_loss           | 88.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 427          |\n",
      "|    ep_rew_mean          | -60.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 482          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 140          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090563875 |\n",
      "|    clip_fraction        | 0.0826       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.981       |\n",
      "|    explained_variance   | 0.419        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.9         |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0095      |\n",
      "|    value_loss           | 117          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 444         |\n",
      "|    ep_rew_mean          | -60.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 483         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009203539 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.99       |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.1        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    value_loss           | 46.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 457          |\n",
      "|    ep_rew_mean          | -59.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 149          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048354967 |\n",
      "|    clip_fraction        | 0.0352       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.989       |\n",
      "|    explained_variance   | 0.188        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 58.6         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00583     |\n",
      "|    value_loss           | 203          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 474         |\n",
      "|    ep_rew_mean          | -57.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 155         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008526113 |\n",
      "|    clip_fraction        | 0.0921      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.939      |\n",
      "|    explained_variance   | 0.654       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.1        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00753    |\n",
      "|    value_loss           | 65.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 481         |\n",
      "|    ep_rew_mean          | -56.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024906611 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.54        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 13.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 496        |\n",
      "|    ep_rew_mean          | -55.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 165        |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01044952 |\n",
      "|    clip_fraction        | 0.0753     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0.581      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 48.8       |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.00343   |\n",
      "|    value_loss           | 65         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 511         |\n",
      "|    ep_rew_mean          | -51.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 466         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 171         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011500914 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.19        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 15.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 531         |\n",
      "|    ep_rew_mean          | -52         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 461         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008242823 |\n",
      "|    clip_fraction        | 0.0559      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.912      |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.8         |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0057     |\n",
      "|    value_loss           | 10.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 545          |\n",
      "|    ep_rew_mean          | -47.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 459          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 182          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058892462 |\n",
      "|    clip_fraction        | 0.028        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.538        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.8         |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00387     |\n",
      "|    value_loss           | 77.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 561         |\n",
      "|    ep_rew_mean          | -46.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 454         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 189         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008127672 |\n",
      "|    clip_fraction        | 0.069       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.6         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.77        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00371    |\n",
      "|    value_loss           | 36.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 577         |\n",
      "|    ep_rew_mean          | -42.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 452         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005580185 |\n",
      "|    clip_fraction        | 0.043       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.877      |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.38        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00266    |\n",
      "|    value_loss           | 11.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 593         |\n",
      "|    ep_rew_mean          | -42         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 449         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 200         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014219698 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.953      |\n",
      "|    explained_variance   | 0.504       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.65        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00994    |\n",
      "|    value_loss           | 56.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 612          |\n",
      "|    ep_rew_mean          | -37.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 447          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 205          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058788955 |\n",
      "|    clip_fraction        | 0.0788       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.898       |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.97         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00538     |\n",
      "|    value_loss           | 10.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 627         |\n",
      "|    ep_rew_mean          | -35.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 448         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 210         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006266861 |\n",
      "|    clip_fraction        | 0.0411      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.766      |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 63.4        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00291    |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 644         |\n",
      "|    ep_rew_mean          | -36.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 448         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 214         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004854993 |\n",
      "|    clip_fraction        | 0.0493      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.799      |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.93        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00261    |\n",
      "|    value_loss           | 57.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 658         |\n",
      "|    ep_rew_mean          | -35.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 444         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 221         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007034745 |\n",
      "|    clip_fraction        | 0.0358      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.876      |\n",
      "|    explained_variance   | 0.582       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.8        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0018     |\n",
      "|    value_loss           | 90.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 668         |\n",
      "|    ep_rew_mean          | -33.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 439         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013200258 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.828      |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.93        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00565    |\n",
      "|    value_loss           | 7.48        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x21b7c67f4f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 🎯 Entraînement du modèle\n",
    "total_timesteps = 100_000\n",
    "\n",
    "# Optionnel : Callback pour sauvegarder des checkpoints auto\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=10_000,\n",
    "    save_path=model_dir,\n",
    "    name_prefix=\"ppo_lunarlander\"\n",
    ")\n",
    "\n",
    "# ⏱️ Lancement de l'entraînement\n",
    "model.learn(total_timesteps=total_timesteps, callback=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c7dc9",
   "metadata": {},
   "source": [
    "### Sauvegarde manuelle du modèle final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d3244fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 Sauvegarde manuelle du modèle entraîné\n",
    "model.save(f\"{model_dir}/ppo_lunarlander_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4fe96a",
   "metadata": {},
   "source": [
    "## 6️⃣ 🔸Évaluation PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb3d14",
   "metadata": {},
   "source": [
    "\n",
    "### Chargement du modèle entraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a4ff40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# 🔁 Chargement du modèle sauvegardé précédemment\n",
    "model_path = \"models/PPO_LunarLander/ppo_lunarlander_final\"\n",
    "model = PPO.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52005d80",
   "metadata": {},
   "source": [
    "### Visualisation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d23f8",
   "metadata": {},
   "source": [
    "\n",
    "Dans le terminal : \n",
    "\n",
    "```bash\n",
    "python test_ppo_lander.py\n",
    "\n",
    "tensorboard --logdir ppo_tensorboard\\PPO_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc94ed",
   "metadata": {},
   "source": [
    "##  7️⃣ 🔹 Entraînement avec DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e1ea11",
   "metadata": {},
   "source": [
    "### Introduction à DQN\n",
    "\n",
    "Deep Q-Network (DQN) est un algorithme de Reinforcement Learning value-based qui combine Q-learning classique avec un réseau de neurones profond (deep neural network).  \n",
    "L’idée est d’utiliser un réseau neuronal pour approximer la fonction Q(s, a), qui estime la valeur d’une action `a` dans un état `s`.  \n",
    "Cela permet de gérer des espaces d’états continus ou très larges, ce qui est le cas pour des environnements complexes comme LunarLander.\n",
    "\n",
    "DQN utilise plusieurs techniques clés pour stabiliser l’apprentissage, notamment :  \n",
    "- Le **replay buffer** pour stocker les transitions et apprendre par mini-batchs, évitant la corrélation entre expériences successives.  \n",
    "- Un **réseau cible** (target network) séparé mis à jour périodiquement pour stabiliser les cibles d’apprentissage.  \n",
    "- La stratégie **ε-greedy** pour un bon équilibre entre exploration et exploitation.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf9d70b",
   "metadata": {},
   "source": [
    "### Paramètres clés de DQN\n",
    "\n",
    "- `learning_rate` : taux d’apprentissage du réseau neuronal. Un petit taux assure une convergence stable.  \n",
    "- `buffer_size` : taille maximale du replay buffer. Plus grand permet d’avoir plus de diversité d’expériences.  \n",
    "- `learning_starts` : nombre de pas avant de commencer à entraîner le réseau, pour remplir un peu le buffer.  \n",
    "- `batch_size` : nombre d’échantillons tirés du replay buffer à chaque mise à jour.  \n",
    "- `tau` : paramètre de mise à jour du réseau cible (soft update).  \n",
    "- `gamma` : facteur de discount, contrôle l’importance des récompenses futures.  \n",
    "- `train_freq` : fréquence (en nombre de pas) à laquelle le réseau est entraîné.  \n",
    "- `exploration_fraction` et `exploration_final_eps` : contrôle la décroissance de ε dans la stratégie ε-greedy, pour passer d’exploration à exploitation.\n",
    "\n",
    "Ces paramètres influencent fortement la qualité et la vitesse d’apprentissage.\n",
    "\n",
    "---\n",
    "\n",
    "Dans la prochaine étape, nous allons créer notre agent DQN avec ces paramètres et lancer l’entraînement sur LunarLander."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3836f5",
   "metadata": {},
   "source": [
    "### Création de l’agent DQN et entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb91dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Modèle DQN sauvegardé sous 'dqn_lunarlander_model.zip'\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "model_dir = \"models/dqn_LunarLander\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Création de l'agent DQN avec des paramètres clés\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=50000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=32,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.02,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./dqn_tensorboard/\"\n",
    ")\n",
    "\n",
    "\n",
    "# Sauvegarde du modèle entraîné\n",
    "model.save(\"dqn_lunarlander_model\")\n",
    "print(\"Modèle DQN sauvegardé sous 'dqn_lunarlander_model.zip'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3309d60",
   "metadata": {},
   "source": [
    "### Lancement de l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5edab907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./dqn_tensorboard/DQN_1\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -195     |\n",
      "|    exploration_rate | 0.959    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 757      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 417      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 96       |\n",
      "|    ep_rew_mean      | -242     |\n",
      "|    exploration_rate | 0.925    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1190     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 768      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 93.8     |\n",
      "|    ep_rew_mean      | -186     |\n",
      "|    exploration_rate | 0.89     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 1259     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1126     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.75     |\n",
      "|    n_updates        | 31       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | -207     |\n",
      "|    exploration_rate | 0.842    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1118     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 1610     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.69     |\n",
      "|    n_updates        | 152      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -185     |\n",
      "|    exploration_rate | 0.801    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 1078     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 2035     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.88     |\n",
      "|    n_updates        | 258      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -175     |\n",
      "|    exploration_rate | 0.755    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1042     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 2498     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.552    |\n",
      "|    n_updates        | 374      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 112      |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.694    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 999      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 3123     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.98     |\n",
      "|    n_updates        | 530      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 116      |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.637    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 953      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 3706     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.652    |\n",
      "|    n_updates        | 676      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 121      |\n",
      "|    ep_rew_mean      | -168     |\n",
      "|    exploration_rate | 0.572    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 916      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 4371     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.798    |\n",
      "|    n_updates        | 842      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 126      |\n",
      "|    ep_rew_mean      | -164     |\n",
      "|    exploration_rate | 0.508    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 898      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 5022     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.09     |\n",
      "|    n_updates        | 1005     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 133      |\n",
      "|    ep_rew_mean      | -163     |\n",
      "|    exploration_rate | 0.425    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 874      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 5872     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.445    |\n",
      "|    n_updates        | 1217     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 152      |\n",
      "|    ep_rew_mean      | -158     |\n",
      "|    exploration_rate | 0.286    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 783      |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 7281     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.659    |\n",
      "|    n_updates        | 1570     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 162      |\n",
      "|    ep_rew_mean      | -165     |\n",
      "|    exploration_rate | 0.173    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 738      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 8437     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.87     |\n",
      "|    n_updates        | 1859     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 185      |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 654      |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 10372    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.503    |\n",
      "|    n_updates        | 2342     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 193      |\n",
      "|    ep_rew_mean      | -181     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 642      |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 11571    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.29     |\n",
      "|    n_updates        | 2642     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 204      |\n",
      "|    ep_rew_mean      | -183     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 629      |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 13072    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.798    |\n",
      "|    n_updates        | 3017     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 220      |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 596      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 14982    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.403    |\n",
      "|    n_updates        | 3495     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 230      |\n",
      "|    ep_rew_mean      | -169     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 588      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 16568    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.1      |\n",
      "|    n_updates        | 3891     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 235      |\n",
      "|    ep_rew_mean      | -165     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 590      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 17880    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.456    |\n",
      "|    n_updates        | 4219     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -161     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 565      |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 19834    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.443    |\n",
      "|    n_updates        | 4708     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 256      |\n",
      "|    ep_rew_mean      | -156     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 555      |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 21514    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.88     |\n",
      "|    n_updates        | 5128     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 256      |\n",
      "|    ep_rew_mean      | -155     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 554      |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 22523    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.88     |\n",
      "|    n_updates        | 5380     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 258      |\n",
      "|    ep_rew_mean      | -156     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 556      |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 23742    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.1      |\n",
      "|    n_updates        | 5685     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 274      |\n",
      "|    ep_rew_mean      | -152     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 550      |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 26281    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.75     |\n",
      "|    n_updates        | 6320     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 287      |\n",
      "|    ep_rew_mean      | -156     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 535      |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 28683    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.928    |\n",
      "|    n_updates        | 6920     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 304      |\n",
      "|    ep_rew_mean      | -150     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 524      |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 30861    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.597    |\n",
      "|    n_updates        | 7465     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 324      |\n",
      "|    ep_rew_mean      | -140     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 511      |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 33166    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.813    |\n",
      "|    n_updates        | 8041     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 344      |\n",
      "|    ep_rew_mean      | -134     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 505      |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 35504    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.646    |\n",
      "|    n_updates        | 8625     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 371      |\n",
      "|    ep_rew_mean      | -128     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 483      |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 38749    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.949    |\n",
      "|    n_updates        | 9437     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 380      |\n",
      "|    ep_rew_mean      | -123     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 486      |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 40032    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.45     |\n",
      "|    n_updates        | 9757     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 395      |\n",
      "|    ep_rew_mean      | -123     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 486      |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 41994    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.228    |\n",
      "|    n_updates        | 10248    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 408      |\n",
      "|    ep_rew_mean      | -117     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 484      |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 43883    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.534    |\n",
      "|    n_updates        | 10720    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 417      |\n",
      "|    ep_rew_mean      | -108     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 485      |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 45424    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 11105    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 432      |\n",
      "|    ep_rew_mean      | -110     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 483      |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 47596    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.349    |\n",
      "|    n_updates        | 11648    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 445      |\n",
      "|    ep_rew_mean      | -105     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 485      |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 49537    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.649    |\n",
      "|    n_updates        | 12134    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 450      |\n",
      "|    ep_rew_mean      | -101     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 487      |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 50858    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.843    |\n",
      "|    n_updates        | 12464    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 471      |\n",
      "|    ep_rew_mean      | -98.5    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 476      |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 54407    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.285    |\n",
      "|    n_updates        | 13351    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 481      |\n",
      "|    ep_rew_mean      | -88.1    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 473      |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 56555    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.477    |\n",
      "|    n_updates        | 13888    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 501      |\n",
      "|    ep_rew_mean      | -79      |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 463      |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 60426    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.32     |\n",
      "|    n_updates        | 14856    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 526      |\n",
      "|    ep_rew_mean      | -71.9    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 454      |\n",
      "|    time_elapsed     | 141      |\n",
      "|    total_timesteps  | 64207    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.511    |\n",
      "|    n_updates        | 15801    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 539      |\n",
      "|    ep_rew_mean      | -67.2    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 449      |\n",
      "|    time_elapsed     | 148      |\n",
      "|    total_timesteps  | 66957    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.433    |\n",
      "|    n_updates        | 16489    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 560      |\n",
      "|    ep_rew_mean      | -69.5    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 440      |\n",
      "|    time_elapsed     | 161      |\n",
      "|    total_timesteps  | 70957    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.385    |\n",
      "|    n_updates        | 17489    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 584      |\n",
      "|    ep_rew_mean      | -73.6    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 437      |\n",
      "|    time_elapsed     | 171      |\n",
      "|    total_timesteps  | 74957    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.646    |\n",
      "|    n_updates        | 18489    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 611      |\n",
      "|    ep_rew_mean      | -75.5    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 431      |\n",
      "|    time_elapsed     | 183      |\n",
      "|    total_timesteps  | 78957    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.347    |\n",
      "|    n_updates        | 19489    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 631      |\n",
      "|    ep_rew_mean      | -77.2    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 429      |\n",
      "|    time_elapsed     | 193      |\n",
      "|    total_timesteps  | 82957    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.527    |\n",
      "|    n_updates        | 20489    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 654      |\n",
      "|    ep_rew_mean      | -80.1    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 424      |\n",
      "|    time_elapsed     | 204      |\n",
      "|    total_timesteps  | 86957    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.475    |\n",
      "|    n_updates        | 21489    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 684      |\n",
      "|    ep_rew_mean      | -79      |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 425      |\n",
      "|    time_elapsed     | 213      |\n",
      "|    total_timesteps  | 90957    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.221    |\n",
      "|    n_updates        | 22489    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 712      |\n",
      "|    ep_rew_mean      | -77.8    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 422      |\n",
      "|    time_elapsed     | 224      |\n",
      "|    total_timesteps  | 94957    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.355    |\n",
      "|    n_updates        | 23489    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 724      |\n",
      "|    ep_rew_mean      | -79.6    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 421      |\n",
      "|    time_elapsed     | 233      |\n",
      "|    total_timesteps  | 98639    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.417    |\n",
      "|    n_updates        | 24409    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x22ce069c520>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "# 🎯 Entraînement du modèle\n",
    "total_timesteps = 100_000\n",
    "\n",
    "# Optionnel : Callback pour sauvegarder des checkpoints auto\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=10_000,\n",
    "    save_path=model_dir,\n",
    "    name_prefix=\"DQN_lunarlander\"\n",
    ")\n",
    "\n",
    "# ⏱️ Lancement de l'entraînement\n",
    "model.learn(total_timesteps=total_timesteps, callback=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e43f3a2",
   "metadata": {},
   "source": [
    "### Sauvegarde manuelle du modèle final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44ca5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 Sauvegarde manuelle du modèle entraîné\n",
    "model.save(f\"{model_dir}/dqn_lunarlander_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70d1b1c",
   "metadata": {},
   "source": [
    "## 8️⃣ 🔹 Évaluation DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1edced5",
   "metadata": {},
   "source": [
    "\n",
    "### Chargement du modèle entraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf4e3b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# 🔁 Chargement du modèle sauvegardé précédemment\n",
    "model_path = \"models/dqn_LunarLander/DQN_lunarlander_final\"\n",
    "model = DQN.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8f5fb",
   "metadata": {},
   "source": [
    "\n",
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c1ab0",
   "metadata": {},
   "source": [
    "Dans le terminal \n",
    "```bash\n",
    "python test_dqn_lander.py\n",
    "tensorboard --logdir dqn_tensorboard\\DQN_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a3215",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_lander",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
